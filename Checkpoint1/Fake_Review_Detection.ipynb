{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oou8i45oDlmW",
        "outputId": "7f8508f5-3b18-44d7-c2a6-357e737b743b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'fakeReviewData.csv'\n",
        "raw_data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(raw_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values by removing rows with any NaN values\n",
        "cleaned_data = raw_data.dropna()\n",
        "\n",
        "# Removing duplicate rows\n",
        "cleaned_data = cleaned_data.drop_duplicates()\n",
        "\n",
        "print(cleaned_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdahs2VOLl0g",
        "outputId": "46cbe69d-3000-43e1-c1d1-a15b1915de10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "\n",
        "    # Converting text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation, special characters, and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Applying the normalization function to the text column\n",
        "cleaned_data['text_'] = cleaned_data['text_'].apply(normalize_text)\n",
        "\n",
        "print(cleaned_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R6oSCVKLuUT",
        "outputId": "e6b93356-05de-40e0-c76f-501a12eebfca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  love this  well made sturdy and very comfortab...  \n",
            "1  love it a great upgrade from the original  ive...  \n",
            "2  this pillow saved my back i love the look and ...  \n",
            "3  missing information on how to use it but it is...  \n",
            "4  very nice set good quality we have had the set...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Downloading the NLTK tokenizer models (only needs to be done once)\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenizing the 'text_' column using NLTK's word_tokenize function\n",
        "cleaned_data['tokens'] = cleaned_data['text_'].apply(word_tokenize)\n",
        "\n",
        "print(cleaned_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQmxYgytYHKP",
        "outputId": "0d9f73e2-583d-4b86-ff6e-1dbd981a0714"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, this, well, made, sturdy, and, very, co...  \n",
            "1  [love, it, a, great, upgrade, from, the, origi...  \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...  \n",
            "3  [missing, information, on, how, to, use, it, b...  \n",
            "4  [very, nice, set, good, quality, we, have, had...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading stopwords list and tokenizer models (only needs to be done once)\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for word in tokens:\n",
        "\n",
        "        # Check if the word is not in the set of stopwords\n",
        "        if word not in stop_words:\n",
        "\n",
        "            # Add the word to the filtered list if it's not a stopword\n",
        "            filtered_tokens.append(word)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "cleaned_data['tokens'] = cleaned_data['tokens'].apply(remove_stopwords)\n",
        "\n",
        "print(cleaned_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziLsNy7OYKKw",
        "outputId": "e64f0ea7-9cf5-4160-ffb8-b268a1932559"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, ive, mine, co...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Downloading the WordNet lemmatizer models (only needs to be done once)\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(tokens):\n",
        "\n",
        "    # Initialize an empty list to store lemmatized tokens\n",
        "    lemmatized_tokens = []\n",
        "\n",
        "    # Iterate over each token in the list\n",
        "    for word in tokens:\n",
        "\n",
        "        # Lemmatize the word and add it to the lemmatized_tokens list\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "# Applying lemmatization to the 'tokens' column\n",
        "cleaned_data['tokens'] = cleaned_data['tokens'].apply(apply_lemmatization)\n",
        "\n",
        "print(cleaned_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfYr6m82YKIA",
        "outputId": "59221808-7bd8-4ea7-9f7f-78bcefd34a8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, well, make, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, ive, mine, co...  \n",
            "2     [pillow, save, back, love, look, feel, pillow]  \n",
            "3    [miss, information, use, great, product, price]  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Step 1: Prepare Corpus\n",
        "# Extract the 'tokens' column as a list of tokenized sentences\n",
        "corpus = cleaned_data['tokens'].tolist()\n",
        "\n",
        "# Step 2: Train Word2Vec Model\n",
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Step 3: Create Sentence Vectors\n",
        "# Function to compute the average word vector for a list of tokens\n",
        "\n",
        "def get_sentence_vector(tokens, model):\n",
        "\n",
        "  # Initialize an empty list to store word vectors\n",
        "  vectors = []\n",
        "\n",
        "  # Loop through each word in tokens\n",
        "  for word in tokens:\n",
        "     # Check if the word exists in the Word2Vec model's vocabulary\n",
        "      if word in model.wv:\n",
        "          # Retrieve the word vector and append it to the list\n",
        "          vectors.append(model.wv[word])\n",
        "\n",
        "\n",
        "  if vectors:\n",
        "      return sum(vectors) / len(vectors)\n",
        "  else:\n",
        "      return [0] * model.vector_size  # Return zero vector for empty tokens\n",
        "\n",
        "\n",
        "\n",
        "# Applying the function to compute sentence vectors\n",
        "cleaned_data['sentence_vector'] = cleaned_data['tokens'].apply(lambda x: get_sentence_vector(x, model))\n",
        "\n",
        "# Step 4: Save Processed Data\n",
        "cleaned_data.to_csv(\"processed_fake_review_data.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"processed_fake_review_data.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9RjQ4PqeYKDe",
        "outputId": "0cfe2915-4b74-4fb9-c1e3-334082ef3081"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6cb45795-50f1-42a2-a8a4-645f3a393bb9\", \"processed_fake_review_data.csv\", 78588254)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('processed_fake_review_data.csv');\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxYNsg5KYKA8",
        "outputId": "3107d26e-6a25-446e-f5c2-2a3ac7fc25f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  ['love', 'well', 'make', 'sturdy', 'comfortabl...   \n",
            "1  ['love', 'great', 'upgrade', 'original', 'ive'...   \n",
            "2  ['pillow', 'save', 'back', 'love', 'look', 'fe...   \n",
            "3  ['miss', 'information', 'use', 'great', 'produ...   \n",
            "4  ['nice', 'set', 'good', 'quality', 'set', 'two...   \n",
            "\n",
            "                                     sentence_vector  \n",
            "0  [ 0.18759157 -0.03918396 -0.34630385  0.506291...  \n",
            "1  [-0.20750514 -0.39860207 -0.2768439   0.663765...  \n",
            "2  [ 0.6719271   0.02558001 -0.85576934  0.201202...  \n",
            "3  [-0.14229308  0.21150582 -0.90268445  0.294856...  \n",
            "4  [-0.1311765  -0.27767465 -0.5119664   1.045890...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('good')"
      ],
      "metadata": {
        "id": "wIu_pRu9YJ-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0849f004-2f0b-4bd4-dad6-db289e28a7a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great', 0.7249922156333923),\n",
              " ('decent', 0.7241107225418091),\n",
              " ('awesome', 0.6723101735115051),\n",
              " ('predictablei', 0.6567014455795288),\n",
              " ('enjoyable', 0.6471998691558838),\n",
              " ('excellent', 0.6431293487548828),\n",
              " ('impress', 0.6352933645248413),\n",
              " ('authorit', 0.6327791213989258),\n",
              " ('nice', 0.6307988166809082),\n",
              " ('roomthis', 0.6217061281204224)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dffV8CF9YJ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vtFl2e2bYJ2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4mMd_y_YJz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--7hUTFEYJpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}